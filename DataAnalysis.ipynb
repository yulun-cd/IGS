{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from libpysal.weights import Kernel\n",
    "from esda.moran import Moran\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, cross_val_predict, cross_val_score, train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seaborn theme\n",
    "sns.set_theme(style='darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_gdf = gpd.read_file('data/AQMS_loc.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set buffer zones around each site (1km)\n",
    "loc_gdf['buffer_1km'] = loc_gdf['geometry'].buffer(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_gdf = loc_gdf[['buffer_1km']]\n",
    "buffer_gdf = gpd.GeoDataFrame(buffer_gdf, geometry='buffer_1km')\n",
    "buffer_gdf.to_file('data/buffer.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gsp modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0575 - LH0\n",
    "\n",
    "1065, 1070, 1565, 1570 - TD5\n",
    "\n",
    "2565, 2570, 3065, 3070 - CR8\n",
    "\n",
    "2565, 3065 - ST5\n",
    "\n",
    "2080, 2580 - KC1\n",
    "\n",
    "2580, 2585 - CD1\n",
    "\n",
    "2580 - MY7\n",
    "\n",
    "2580, 3080 - BL0, CD9\n",
    "\n",
    "3080 - CT2, CT3\n",
    "\n",
    "3570, 3575 - HP1, LW2\n",
    "\n",
    "3575, 3580, 4075, 4080 - GN6\n",
    "\n",
    "3580 - TH4\n",
    "\n",
    "4070, 4075, 4570, 4575 - GB0\n",
    "\n",
    "4070, 4075 - GR9, GR4\n",
    "\n",
    "4075, 4575 - GN3\n",
    "\n",
    "5075 - BX9\n",
    "\n",
    "5080 - HV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readin_Gsp(file_name, path='data/OSMM Greenspaces/tq/TQ', suffix='_GreenspaceArea.shp'):\n",
    "    if type(file_name) == str:\n",
    "        gdf = gpd.read_file(path+file_name+suffix)\n",
    "    else:\n",
    "        gdf = pd.concat(gpd.read_file(path+f+suffix) for f in file_name)\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_gdf['Gsp'] = gpd.GeoSeries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_gdf.columns.get_loc('Gsp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Gsp(file_name, index):\n",
    "    gdf = readin_Gsp(file_name)\n",
    "    print('Finish reading in shapefile(s)')\n",
    "    shp = gdf['geometry'].unary_union\n",
    "    print('Finish unary union.')\n",
    "    if type(index) == int:\n",
    "        loc_gdf.iat[index, 4] = shp.intersection(loc_gdf.loc[index, 'buffer_1km'])\n",
    "    elif type(index) == list:\n",
    "        for i in index:\n",
    "            loc_gdf.iat[i, 4] = shp.intersection(loc_gdf.loc[i, 'buffer_1km'])\n",
    "    else:\n",
    "        print('invalid type!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_Gsp('0575', 13)\n",
    "loc_gdf.loc[13, 'Gsp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_Gsp(['1065','1070','1565','1570'], 17)\n",
    "loc_gdf.loc[17, 'Gsp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_Gsp(['2565','2570','3065','3070'], 6)\n",
    "loc_gdf.loc[6, 'Gsp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_Gsp(['2565','3065'], 18)\n",
    "loc_gdf.loc[18, 'Gsp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_Gsp(['2080','2580'], 14)\n",
    "loc_gdf.loc[14, 'Gsp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_Gsp(['2580','2585'], 3)\n",
    "loc_gdf.loc[3, 'Gsp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_Gsp('2580', 20)\n",
    "loc_gdf.loc[20, 'Gsp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_Gsp(['2580','3080'], [1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_gdf.loc[1, 'Gsp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_gdf.loc[2, 'Gsp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_Gsp('3080', [4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_gdf.loc[4, 'Gsp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_gdf.loc[5, 'Gsp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_Gsp(['3570','3575'], [15,16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_gdf.loc[15, 'Gsp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_gdf.loc[16, 'Gsp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_Gsp(['3575','3580','4075','4080'], 9)\n",
    "loc_gdf.loc[9, 'Gsp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_Gsp('3580', 19)\n",
    "loc_gdf.loc[19, 'Gsp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_Gsp(['4070','4075','4570','4575'], 8)\n",
    "loc_gdf.loc[8, 'Gsp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_Gsp(['4075', '4575'], [7, 11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_gdf.loc[7, 'Gsp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_gdf.loc[11, 'Gsp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_Gsp(['4075', '4575'], 10)\n",
    "loc_gdf.loc[10, 'Gsp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_Gsp('5075', 0)\n",
    "loc_gdf.loc[0, 'Gsp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_Gsp('5080', 12)\n",
    "loc_gdf.loc[12, 'Gsp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gsp_gdf = loc_gdf[['siteid','Gsp']]\n",
    "Gsp_gdf = Gsp_gdf.set_geometry('Gsp')\n",
    "Gsp_gdf = Gsp_gdf.set_crs(27700)\n",
    "Gsp_gdf.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gsp_gdf.to_file('data/gsp_buffer_1km.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nRd_gsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gsp_gdf = gpd.read_file('data/gsp_buffer_1km.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gsp_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_gdf['Gsp'] = Gsp_gdf['geometry']\n",
    "loc_gdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Gsp_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in all the data\n",
    "AQMS_df = pd.read_csv('data/hourly.csv')\n",
    "Rd_gdf = gpd.read_file('data/london_Road.shp')\n",
    "cond = pd.read_csv('data/cond_hourly.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rd_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in Rd_gdf['class'].unique():\n",
    "    print('Number of ' + c + ': ', Rd_gdf[Rd_gdf['class'] == c].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all types of roads\n",
    "Rd = {}\n",
    "for c in Rd_gdf['class'].unique():\n",
    "    Rd[c] = Rd_gdf[Rd_gdf['class'] == c].loc[:, 'geometry'].unary_union\n",
    "Rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rd['other'] = unary_union([Rd['Not Classified'], Rd['Unknown']])\n",
    "Rd.pop('Not Classified')\n",
    "Rd.pop('Unknown')\n",
    "Rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Rd_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in Rd.keys():\n",
    "    loc_gdf[key] = loc_gdf['buffer_1km'].intersection(Rd[key])\n",
    "\n",
    "loc_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "loc_gdf.rename(columns={'Unclassified': 'UnC', \n",
    "                        'A Road': 'A',\n",
    "                        'B Road': 'B',\n",
    "                        'Classified Unnumbered': 'CUn',\n",
    "                        'Motorway': 'Mt',\n",
    "                        'other': 'Other'}, inplace=True)\n",
    "\n",
    "Rd_type = loc_gdf.columns[-6:]\n",
    "Rd_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all near-road green spaces\n",
    "for col in Rd_type:\n",
    "    loc_gdf['n'+col+'_Gsp'] = loc_gdf['Gsp'].intersection(loc_gdf[col].buffer(50))\n",
    "\n",
    "loc_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# london boundary read in\n",
    "london = gpd.read_file('data/london_boundary.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise all the sites on the map\n",
    "fig,ax = plt.subplots(1, figsize=(15,13))\n",
    "\n",
    "london.plot(color='lightgrey', ax=ax)\n",
    "loc_gdf['buffer_1km'].plot(color='silver', ax=ax)\n",
    "loc_gdf['geometry'].plot(markersize=10, marker='^', color='blue', \n",
    "                         label='Air quality monitoring site', ax=ax)\n",
    "\n",
    "ax.axis('off')\n",
    "\n",
    "legend=ax.legend(loc='best',shadow=True,fontsize=15)\n",
    "\n",
    "#plt.savefig('sample1.png',facecolor='black',dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some buffers that seem to be very close to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column that specifies the shortest distance of a site to its nearest neighbour\n",
    "loc_gdf['min_dis'] = pd.Series(dtype='float64')\n",
    "for index, row in loc_gdf.iterrows():\n",
    "    dis = []\n",
    "    for i, v in loc_gdf['geometry'].iteritems():\n",
    "        dis.append(row['geometry'].distance(v))\n",
    "    dis.remove(0)\n",
    "    loc_gdf.loc[index, 'min_dis'] = min(dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list sites that are close to each other (within 1.5km)\n",
    "loc_gdf[loc_gdf['min_dis']<=1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check their readings' descriptive statistics\n",
    "AQMS_df[AQMS_df['Site'].isin(['BL0', 'CD9', 'GR4', 'GB0'])].groupby('Site').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_rel(AQMS_df[AQMS_df['Site']=='BL0'].Value.values,\n",
    "                AQMS_df[AQMS_df['Site']=='CD9'].Value.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_rel(AQMS_df[AQMS_df['Site']=='GR4'].Value.values,\n",
    "                AQMS_df[AQMS_df['Site']=='GB0'].Value.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both indicate that we should reject H0, meaning the two datasets are statistically significantly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revmove them from the list\n",
    "#loc_gdf.drop(['BL0','GR4'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get areas and edge lengths of green spaces \n",
    "loc_gdf['Gsp_area'] = loc_gdf['Gsp'].area\n",
    "loc_gdf['Gsp_edge'] = loc_gdf['Gsp'].length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get road lengths of each type and nRd gsp area percentages\n",
    "for col in Rd_type:\n",
    "    loc_gdf[col+'_len'] = loc_gdf[col].length\n",
    "    loc_gdf['n'+col+'_Gsp_per_'+col+'_len'] = loc_gdf['n'+ col+'_Gsp'].area / loc_gdf[col+'_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_gdf['Gsp_per_tRd_len'] = loc_gdf['Gsp_area'] / loc_gdf[[col+'_len' for col in Rd_type]].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_gdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge PM reading and site geogemetry data\n",
    "df = pd.merge(AQMS_df, loc_gdf, left_on='Site', right_on='siteid')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop irrelevant columns\n",
    "df.drop(['siteid', 'sitename', 'geometry', 'buffer_1km', 'Gsp', 'min_dis'], axis=1, inplace=True)\n",
    "df.drop(Rd_type , axis=1, inplace=True)\n",
    "df.drop(['n'+rd+'_Gsp' for rd in Rd_type], axis=1, inplace=True)\n",
    "df.drop([rd+'_len' for rd in Rd_type], axis=1, inplace=True)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many null values in `nMt_Gsp_per_Mt_len`.\n",
    "\n",
    "Because only one site has near motorway.\n",
    "\n",
    "Remove the variable would be the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('nMt_Gsp_per_Mt_len', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some null values in `nB_Gsp_per_B_len` and `nCUn_Gsp_per_CUn_len`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the null values to zero\n",
    "df.fillna(0, inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with conditional variables\n",
    "df = df.merge(cond, on='ReadingDateTime')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_names = df.columns[5:10].tolist()\n",
    "exp_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = exp_names + ['Gsp_edge', 'Gsp_per_tRd_len']\n",
    "var_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_names = df.columns[-3:].tolist()\n",
    "cond_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[var_names + cond_names].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('temp_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporarily save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('temp_data.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covert the DateTime column to numpy.datetime variable\n",
    "df['ReadingDateTime'] = pd.to_datetime(df['ReadingDateTime'], format=\"%d/%m/%Y %H:%M\")\n",
    "df.rename(columns={'ReadingDateTime':'DateTime'}, inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = df['DateTime'].dt.date\n",
    "\n",
    "sns.scatterplot(x=df['Date'].unique(), y=df.groupby('Date').mean()['Value'])\n",
    "\n",
    "plt.axhline(y=15, color='red', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of date above WHO guideline\n",
    "(df.groupby('Date').mean()['Value']>15).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annual mean for each site\n",
    "df.groupby('Site').mean()['Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_names = df.columns[5:10].tolist()\n",
    "var_names = exp_names + ['Gsp_edge', 'Gsp_per_tRd_len']\n",
    "cond_names = df.columns[11:14].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_gdf = loc_gdf.set_index('siteid')\n",
    "loc_gdf = pd.merge(df.groupby('Site').mean()[var_names], loc_gdf, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel weight matrix for the sites\n",
    "weight = Kernel.from_dataframe(loc_gdf, geom_col='geometry', function='gaussian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in var_names:\n",
    "    moran_temp = Moran(loc_gdf[var].values, weight)\n",
    "    print(\"Global Moran's I for \" + var + ' is ', round(moran_temp.I, 5), \n",
    "          ' p-value: ', round(moran_temp.p_norm, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Value'].hist(bins=list(range(40)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_Value'] = np.log(df['Value'])\n",
    "df['log_Value'].hist(bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[var_names + cond_names].hist(bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_exp_names = exp_names.copy()\n",
    "t_var_names = var_names.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log(df['nA_Gsp_per_A_len']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_nA_Gsp_per_A_len'] = np.log(df['nA_Gsp_per_A_len'])\n",
    "t_exp_names[1] = 'log_nA_Gsp_per_A_len'\n",
    "t_var_names[1] = 'log_nA_Gsp_per_A_len'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.sqrt(df['nB_Gsp_per_B_len']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sqrt_nB_Gsp_per_B_len'] = np.sqrt(df['nB_Gsp_per_B_len'])\n",
    "t_exp_names[2] = 'sqrt_nB_Gsp_per_B_len'\n",
    "t_var_names[2] = 'sqrt_nB_Gsp_per_B_len'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log(df['nOther_Gsp_per_Other_len']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_nOther_Gsp_per_Other_len'] = np.log(df['nOther_Gsp_per_Other_len'])\n",
    "t_exp_names[4] = 'log_nOther_Gsp_per_Other_len'\n",
    "t_var_names[4] = 'log_nOther_Gsp_per_Other_len'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.sqrt(df['Gsp_per_tRd_len']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sqrt_Gsp_per_tRd_len'] = np.sqrt(df['Gsp_per_tRd_len'])\n",
    "t_var_names[6] = 'sqrt_Gsp_per_tRd_len'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['Prec_mean']>0).sum()/21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is 629 out of 8760 hours in 2019 recorded raining, which is only around 7% of the time. Hence it would be better to use a categorical data (0 being not raining and 1 being raining) to represent the weather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Prec_mean'] = (df['Prec_mean']>0).astype(int)\n",
    "df['Prec_mean'].sum()/21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hour'] = df['DateTime'].dt.hour\n",
    "df.groupby('hour').mean()['Value'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dayofweek'] = df['DateTime'].dt.dayofweek\n",
    "df.groupby('dayofweek').mean()['Value'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dayofmonth'] = df['DateTime'].dt.day\n",
    "df.groupby('dayofmonth').mean()['Value'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_importance(reg, features, target, feature_names, rep=50, method='r2'):\n",
    "    mean = []\n",
    "    std = []\n",
    "    importance = permutation_importance(reg, features, target, n_repeats=rep,\n",
    "                                        random_state=25, scoring=method)\n",
    "    for i in range(len(feature_names)):\n",
    "        mean.append(round(importance.importances_mean[i], 5))\n",
    "        std.append(round(importance.importances_std[i], 5))\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_score(reg, features, target, iter=50, split=10, method='r2'):\n",
    "    score = []\n",
    "    for i in range(iter):\n",
    "        kf = KFold(n_splits=split, shuffle=True, random_state=i)\n",
    "        cv = cross_val_score(reg, features, target, cv=kf, scoring=method).tolist()\n",
    "        score = score + cv\n",
    "    \n",
    "    return (np.mean(score), np.std(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "t_var = t_var_names + cond_names\n",
    "\n",
    "ap_X = df[t_var].values\n",
    "ap_y = df['log_Value'].values\n",
    "\n",
    "ap_X_train, ap_X_test, ap_y_train, ap_y_test = train_test_split(ap_X, ap_y, \n",
    "                                                                shuffle=True, \n",
    "                                                                random_state=25)\n",
    "\n",
    "ap_X_train = scaler.fit_transform(ap_X_train)\n",
    "ap_X_test = scaler.fit_transform(ap_X_test)\n",
    "\n",
    "reg.fit(ap_X_train, ap_y_train)\n",
    "\n",
    "get_importance(reg, ap_X_test, ap_y_test, t_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.score(ap_X_test, ap_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moran = []\n",
    "for time in df['DateTime'].unique():\n",
    "    moran_temp = Moran(df[df['DateTime']==time].log_Value.values, weight)\n",
    "    moran.append([round(moran_temp.I, 5), round(moran_temp.p_norm, 5)])\n",
    "moran_df = pd.DataFrame(df['DateTime'].unique(), columns=['DateTime'])\n",
    "moran_df[['moran', 'p-value']] = moran\n",
    "moran_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moran_df['hour'] = moran_df['DateTime'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(4, 6, figsize=(24,16))\n",
    "i = 0\n",
    "for hour in range(24):\n",
    "    sns.lineplot(x=moran_df['DateTime'].dt.date.unique(), \n",
    "                 y=moran_df[moran_df['hour']==hour].moran.values, \n",
    "                 ax=ax[i//6, i%6], linewidth=1)\n",
    "    i+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_fi = []\n",
    "h_score = []\n",
    "h_coef = []\n",
    "for hour in df['hour'].unique():\n",
    "    X = df[df['hour']==hour].loc[:,t_var].values\n",
    "    y = df[df['hour']==hour].loc[:,'log_Value'].values\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, random_state=25)\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    reg.fit(X_train, y_train)\n",
    "    \n",
    "    fi_mean, fi_std = get_importance(reg, X_test, y_test, feature_names=t_var)\n",
    "    h_fi.append(fi_mean + fi_std)\n",
    "    \n",
    "    h_score.append(reg.score(X_test, y_test))\n",
    "    \n",
    "    coef = reg.coef_.tolist()\n",
    "    coef.append(reg.intercept_)\n",
    "    h_coef.append(coef)\n",
    "    \n",
    "h_fi = pd.DataFrame(h_fi, columns=['fi_' + elem for elem in t_var] + ['fi_std_' + elem for elem in t_var])\n",
    "h_score = pd.DataFrame(h_score, columns=['r2'])\n",
    "h_coef = pd.DataFrame(h_coef, columns=t_var+['intercept'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_reg = pd.concat([h_coef, h_score, h_fi], axis=1)\n",
    "h_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_reg['r2'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 6, figsize=(24, 16))\n",
    "i = 0\n",
    "for hour in range(24):\n",
    "    g = sns.barplot(x=['fi_' + elem for elem in t_var], y=h_reg.loc[hour, ['fi_' + elem for elem in t_var]],\n",
    "                    ax=ax[i//6, i%6])\n",
    "    g.set(xticklabels=[])\n",
    "    i += 1\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a new column for month information\n",
    "df['month'] = df['DateTime'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moran_df['month'] = moran_df['DateTime'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(3, 4, figsize=(16,12))\n",
    "i = 0\n",
    "for month in range(1,13):\n",
    "    sns.lineplot(x='DateTime', y='moran', data=moran_df[moran_df['month']==month],\n",
    "                 ax=ax[i//4, i%4], linewidth=1)\n",
    "    i+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_fi = []\n",
    "m_score = []\n",
    "m_coef = []\n",
    "for month in df['month'].unique():\n",
    "    X = df[df['month']==month].loc[:, t_var].values\n",
    "    y = df[df['month']==month].loc[:, 'log_Value'].values\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, random_state=25)\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    reg.fit(X_train, y_train)\n",
    "    \n",
    "    fi_mean, fi_std = get_importance(reg, X_test, y_test, feature_names=t_var)\n",
    "    m_fi.append(fi_mean + fi_std)\n",
    "\n",
    "    m_score.append(reg.score(X_test, y_test))\n",
    "    \n",
    "    coef = reg.coef_.tolist()\n",
    "    coef.append(reg.intercept_)\n",
    "    m_coef.append(coef)\n",
    "    \n",
    "m_fi = pd.DataFrame(m_fi, columns=['fi_' + elem for elem in t_var] + ['fi_std_' + elem for elem in t_var])\n",
    "m_score = pd.DataFrame(m_score, columns=['r2'])\n",
    "m_coef = pd.DataFrame(m_coef, columns=t_var+['intercept'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_reg = pd.concat([m_coef, m_score, m_fi], axis=1)\n",
    "m_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_reg['r2'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 4, figsize=(16, 12))\n",
    "i = 0\n",
    "for month in range(1,13):\n",
    "    g = sns.barplot(x=['fi_' + elem for elem in t_var], y=m_reg.loc[month-1, ['fi_' + elem for elem in t_var]],\n",
    "                    ax=ax[i//4, i%4])\n",
    "    g.set(xticklabels=[])\n",
    "    i += 1\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('month').mean()['Value'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_period = df[df['month'].isin([1, 2, 4])].drop(['hour','dayofweek','dayofmonth','month'],axis=1)\n",
    "low_period = df[~df['month'].isin([1, 2, 4])].drop(['hour','dayofweek','dayofmonth','month'],axis=1)\n",
    "\n",
    "print('high period: '+str(high_period.shape)+'\\nlow period: '+str(low_period.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_X = high_period[t_var].values\n",
    "hp_y = high_period['log_Value'].values\n",
    "\n",
    "hp_X_train, hp_X_test, hp_y_train, hp_y_test = train_test_split(hp_X, hp_y, shuffle=True, random_state=25)\n",
    "\n",
    "hp_X_train = scaler.fit_transform(hp_X_train)\n",
    "hp_X_test = scaler.fit_transform(hp_X_test)\n",
    "\n",
    "reg.fit(hp_X_train, hp_y_train)\n",
    "\n",
    "get_importance(reg, hp_X_test, hp_y_test, feature_names=t_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.score(hp_X_test, hp_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_.tolist() + [reg.intercept_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_X = low_period[t_var].values\n",
    "lp_y = low_period['log_Value'].values\n",
    "\n",
    "lp_X_train, lp_X_test, lp_y_train, lp_y_test = train_test_split(lp_X, lp_y, shuffle=True, random_state=25)\n",
    "lp_X_train = scaler.fit_transform(lp_X_train)\n",
    "lp_X_test = scaler.fit_transform(lp_X_test)\n",
    "reg.fit(lp_X_train, lp_y_train)\n",
    "\n",
    "get_importance(reg, lp_X_test, lp_y_test, feature_names=t_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.score(lp_X_test, lp_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_.tolist() + [reg.intercept_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmean_df = df.groupby(['hour', 'Site']).mean()\n",
    "hmean_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmean_df.drop(['bp_mean', 'tmp_mean', 'Prec_mean', 'dayofweek', 'dayofmonth', 'month'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(hmean_df[['log_Value']+var_names].corr().round(4),annot=True,fmt='.3f',cmap='magma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corr(df,iter_range,method='pearson',features=var_names,target='log_Value'):\n",
    "    result=[]\n",
    "    for index in iter_range:\n",
    "        result.append(df.loc[(index,)].corr(method=method).loc[features,target])\n",
    "    result=np.asarray(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moran(df,iter_range,w=weight,target='log_Value'):\n",
    "    result=[]\n",
    "    for index in iter_range:\n",
    "        result.append(Moran(df.loc[(index,),target].values,w).I)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv(df, reg, features, target, iter=100, splits=3):\n",
    "    cv_r2 = []\n",
    "    cv_tau = []\n",
    "\n",
    "    for i in range(iter):\n",
    "        kf = KFold(n_splits=splits, shuffle=True, random_state=i)\n",
    "        cvprd = cross_val_predict(reg, features, target, cv=kf)\n",
    "        \n",
    "        r = stats.pearsonr(target,cvprd)[0]\n",
    "        t, p_value = stats.kendalltau(target, cvprd)\n",
    "        \n",
    "        cv_r2.append(r**2)\n",
    "        cv_tau.append(t)\n",
    "\n",
    "    return [round(np.mean(cv_r2),5), round(np.std(cv_r2),5), round(np.mean(cv_tau),5), round(np.std(cv_tau),5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reg_info(df, iter_range, features=var_names, target='log_Value', cv_split=3, reg=LinearRegression()):\n",
    "    result = []\n",
    "    for index in iter_range:\n",
    "        X = df.loc[(index,), features].values        \n",
    "        y = df.loc[(index,), target].values\n",
    "        #X = scaler.fit_transform(X)\n",
    "        \n",
    "        reg.fit(X, y)\n",
    "        coef = reg.coef_.tolist()\n",
    "        cv = get_cv(df, reg, X, y, splits=cv_split)\n",
    "        coef = coef + cv\n",
    "        importance, std = get_importance(reg, X, y, features)\n",
    "        coef = coef + importance + std\n",
    "        result.append(coef)\n",
    "    result = pd.DataFrame(result, columns=features+['cv_r2','r2_std','cv_tau','tau_std']+\n",
    "                          ['fi_'+var for var in features]+['std_fi_'+var for var in features])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmean_corr=get_corr(hmean_df,range(24))\n",
    "sns.lineplot(data=hmean_corr,legend=False)\n",
    "plt.legend(labels=var_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmean_corr_sp=get_corr(hmean_df,range(24),method='spearman')\n",
    "sns.lineplot(data=hmean_corr_sp,legend=False)\n",
    "plt.legend(labels=var_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmean_moran=get_moran(hmean_df,range(24))\n",
    "sns.lineplot(x=range(24),y=hmean_moran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmean_reg=get_reg_info(hmean_df,range(24), cv_split=4)\n",
    "sns.lineplot(data=hmean_reg[['cv_r2','cv_tau']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 6, figsize=(24, 16))\n",
    "i = 0\n",
    "for hour in range(24):\n",
    "    g = sns.barplot(x=['fi_' + elem for elem in var_names], \n",
    "                    y=hmean_reg.loc[hour, ['fi_' + elem for elem in var_names]],\n",
    "                    ax=ax[i//6, i%6])\n",
    "    g.set(xticklabels=[])\n",
    "    i += 1\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmean_df=df.groupby(['month','Site']).mean()\n",
    "mmean_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmean_df.drop(['bp_mean','tmp_mean','Prec_mean','hour','dayofweek','dayofmonth'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmean_corr=get_corr(mmean_df,range(1,13))\n",
    "sns.lineplot(data=mmean_corr,legend=False)\n",
    "plt.legend(labels=var_names,loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmean_corr_sp=get_corr(mmean_df,range(1,13),method='spearman')\n",
    "sns.lineplot(data=mmean_corr_sp,legend=False)\n",
    "plt.legend(labels=var_names,loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmean_moran=get_moran(mmean_df,range(1,13))\n",
    "sns.lineplot(x=range(1,13),y=mmean_moran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmean_reg = get_reg_info(mmean_df,range(1,13),cv_split=2)\n",
    "sns.lineplot(data=mmean_reg[['cv_r2','cv_tau']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 4, figsize=(16, 12))\n",
    "i = 0\n",
    "for month in range(1,13):\n",
    "    g = sns.barplot(x=['fi_' + elem for elem in exp_names], \n",
    "                    y=mmean_reg.loc[month-1, ['fi_' + elem for elem in exp_names]],\n",
    "                    ax=ax[i//4, i%4])\n",
    "    g.set(xticklabels=[])\n",
    "    i += 1\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify high period and low period\n",
    "high = df[df['month'].isin([1,2,4])].groupby('Site').mean()\n",
    "low = df[~df['month'].isin([1,2,4])].groupby('Site').mean()\n",
    "\n",
    "print('high: '+str(high.shape)+'\\nlow: '+str(low.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(high[['log_Value']+var_names].corr().round(4),annot=True,fmt='.4f',cmap='magma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_moran=Moran(high['log_Value'].values,weight)\n",
    "round(high_moran.I,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_high=LinearRegression()\n",
    "y_high = high['log_Value'].values\n",
    "x_high = high[var_names].values\n",
    "\n",
    "x_high = scaler.fit_transform(x_high)\n",
    "reg_high.fit(x_high, y_high)\n",
    "reg_high.score(x_high,y_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_high = reg_high.predict(x_high)\n",
    "\n",
    "r = stats.pearsonr(y_high, prd_high)[0]\n",
    "r2 = r**2\n",
    "t, p_value = stats.kendalltau(y_high, prd_high)\n",
    "print('r2 (obs): ', round(r2, 5))\n",
    "print('tau (obs): ', round(t, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(low[['Value']+exp_names].corr().round(4),annot=True,fmt='.4f',cmap='magma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_moran=Moran(low['Value'].values,weight)\n",
    "round(low_moran.I,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_low = LinearRegression()\n",
    "y_low = low['Value'].values\n",
    "x_low = low[exp_names].values\n",
    "reg_low.fit(x_low, y_low)\n",
    "prd_low = reg_low.predict(x_low)\n",
    "\n",
    "r = low['Value'].corr(pd.Series(prd_low))\n",
    "r2 = r**2\n",
    "t = low['Value'].corr(pd.Series(prd_low), method='kendall')\n",
    "print(\"r2 (cv): \", round(r2,3))   \n",
    "print(\"tau (cv): \", round(t,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(all[['Value']+exp_names].corr().round(4),annot=True,fmt='.4f',cmap='magma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all=df.groupby('Site').mean()\n",
    "all_moran=Moran(all['Value'].values,weight)\n",
    "round(all_moran.I,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all=(all.groupby('Site').mean()['Value'].values)\n",
    "x_all=(all.groupby('Site').mean()[exp_names].values)\n",
    "\n",
    "reg.fit(x_all,y_all)\n",
    "reg.score(x_all,y_all)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "203d01a3458eace725dddcfcdb2c604435d077f4f6685c100707a5d3f77ea293"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('sds2021': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
